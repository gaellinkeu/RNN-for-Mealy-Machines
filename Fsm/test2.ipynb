{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# Accuracy ne prenant pas en compte les charactères complétés\n",
    "\n",
    "# Remove this when the cosineSimilarity will be added\n",
    "def cosineSimilarity(h1, h2):\n",
    "    return 2.3\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=2):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "# Fonction qui imitte le comportement du réseau de neurone\n",
    "def get_hidden_state (word):\n",
    "    prec = 0.005\n",
    "    output = []\n",
    "    \n",
    "    for i, char in enumerate(word):\n",
    "        if char == \"a\":\n",
    "            prec = 0.02*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        elif char == \"b\":\n",
    "            prec = 0.03*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        elif char == \"e\":\n",
    "            prec = 0.005*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        else:\n",
    "            prec = 0.05*(i+1) + prec\n",
    "            output.append(prec)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_data(filepath):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    #print(lines[:3])\n",
    "\n",
    "\n",
    "    max_length = 0\n",
    "\n",
    "    for line in lines:\n",
    "        res = \"\"\n",
    "        isInput = True\n",
    "        for symbol in line:\n",
    "            if symbol in [',', '\\n']:\n",
    "                if isInput:\n",
    "                    inputs.append(res)\n",
    "                    max_length = len(res) if len(res) > max_length else max_length\n",
    "                    res = \"\"\n",
    "                    isInput = not isInput\n",
    "                    continue\n",
    "                else:\n",
    "                    outputs.append(res)\n",
    "            res += symbol\n",
    "        #print(line)\n",
    "    return inputs, outputs, max_length\n",
    "\n",
    "def merging_checking(st1, st2, k):\n",
    "    similarity = False\n",
    "    consistency = False\n",
    "\n",
    "    # for the similarity, we will merge state1 to state 2 if\n",
    "    # If every input of state1 are in the input set of state2 and\n",
    "    # If for each input state1 input set, we get the same output\n",
    "    # from state1 and state2\n",
    "\n",
    "    if set(st1._outTr.keys()) in set(st2._outTr.keys()):\n",
    "        for char in list(st1._outTr.keys()):\n",
    "            if set(st1._outTr[char].keys()) == set(st1._outTr[char].keys()):\n",
    "                similarity = True\n",
    "\n",
    "    # compute the cosine similarity of the two hidden state value\n",
    "    # If it's greater than k, the consistency constraint in respected\n",
    "    if cosineSimilarity(st1.hidden_state, st2.hidden_state) > k:\n",
    "        consistency = True\n",
    "\n",
    "    return similarity and consistency\n",
    "\n",
    "def class_mapping(label, numb_class = 3):\n",
    "    y_train = []\n",
    "    for x in label:\n",
    "        assert int(x) < numb_class\n",
    "        y_train.append([int(i==int(x)) for i in range(numb_class)])\n",
    "        \n",
    "    return y_train\n",
    "\n",
    "def tokenization(word, num_token = 4):\n",
    "    x_train = []\n",
    "    for x in word:\n",
    "        if x == 'a':\n",
    "            x_train.append(1)\n",
    "        elif x == 'b':\n",
    "            x_train.append(2)\n",
    "        elif x == 'e':\n",
    "            x_train.append(3)\n",
    "        else:\n",
    "            x_train.append(0)\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "def masking(word, pad_char = 'z'):\n",
    "    return [x!=pad_char for x in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Tagger \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "corpus, labels, max_length = get_data('dataset2.txt')\n",
    "corpus_ = [\"e\"+x+\"z\"*(max_length-len(x)) for x in corpus]\n",
    "labels_ = [\"0\"+x+\"2\"*(max_length - len(x)) for x in labels]\n",
    "states = []\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "x_train = np.array([tokenization(x) for x in corpus_])\n",
    "y_train = np.array([class_mapping(x) for x in labels_])\n",
    "mask = np.array([masking(x) for x in corpus_])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Division en jeu de test et jeu d'entrainement\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class obj:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "def change(obj):\n",
    "    obj.x = 4\n",
    "\n",
    "rer = obj(2,1)\n",
    "print(rer.x)\n",
    "change(rer)\n",
    "print(rer.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size = 100\n",
    "embedding_vector_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 10)          40        \n",
      "                                                                 \n",
      " simple_rnn_8 (SimpleRNN)    (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 3)           33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 283\n",
      "Trainable params: 283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(4, embedding_vector_length))\n",
    "model.add(SimpleRNN(10, return_sequences=True))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', ignore_class_accuracy(2)])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 38s 582ms/step - loss: 0.7074 - accuracy: 0.7652 - ignore_accuracy: 0.6348 - val_loss: 0.5537 - val_accuracy: 0.8284 - val_ignore_accuracy: 0.6811\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy', 'ignore_accuracy']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 576ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.3826207 , 0.30852887, 0.30885047],\n",
       "        [0.19417945, 0.5030848 , 0.30273584],\n",
       "        [0.26252916, 0.3877717 , 0.34969908],\n",
       "        [0.31407863, 0.3824414 , 0.30348   ]],\n",
       "\n",
       "       [[0.21838854, 0.43559575, 0.3460157 ],\n",
       "        [0.2548318 , 0.3848924 , 0.36027586],\n",
       "        [0.259653  , 0.40548763, 0.33485934],\n",
       "        [0.39601526, 0.30407542, 0.29990935]],\n",
       "\n",
       "       [[0.3826207 , 0.30852887, 0.30885047],\n",
       "        [0.19417945, 0.5030848 , 0.30273584],\n",
       "        [0.43629703, 0.26407394, 0.29962897],\n",
       "        [0.2424045 , 0.49866024, 0.25893533]],\n",
       "\n",
       "       [[0.3826207 , 0.30852887, 0.30885047],\n",
       "        [0.35004368, 0.37009576, 0.27986068],\n",
       "        [0.3576279 , 0.37119833, 0.27117372],\n",
       "        [0.42016873, 0.3390707 , 0.24076055]],\n",
       "\n",
       "       [[0.3826207 , 0.30852887, 0.30885047],\n",
       "        [0.19417945, 0.5030848 , 0.30273584],\n",
       "        [0.26252916, 0.3877717 , 0.34969908],\n",
       "        [0.49322522, 0.25169548, 0.25507924]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['abbb', 'bbba', 'abab', 'aaaa', 'abba']\n",
    "x_predict = np.array([tokenization(x) for x in data])\n",
    "x_predict = tf.convert_to_tensor(x_predict)\n",
    "model.predict(x_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_weights(model):\n",
    "    print(\"Weights and biases of the layers after setting the new weights and biases: \\n\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        print(layer.name)\n",
    "        print(\"Weights\")\n",
    "        print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "        print(\"Bias\")\n",
    "        print(\"Shape: \",layer.get_weights()[1].shape,'\\n',layer.get_weights()[1],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.42692873, -0.42289996,  0.4447741 , -0.4429699 , -0.17001548,\n",
       "          0.11229646, -0.458028  , -0.01096553, -0.48083076,  0.5543523 ],\n",
       "        [ 0.36340922, -0.18404362,  0.28009933,  0.20886461, -0.00743275,\n",
       "          0.10764097,  0.15501681, -0.37760377,  0.29179093,  0.29020905],\n",
       "        [-0.51575154,  0.40278122,  0.11884367,  0.42618102,  0.28357235,\n",
       "         -0.51602376, -0.1604212 ,  0.09842136, -0.10567806,  0.02250358],\n",
       "        [-0.17689845, -0.50512743, -0.5238936 ,  0.43742135,  0.05745095,\n",
       "         -0.54296374, -0.56734043, -0.32235745,  0.1193771 , -0.08280785],\n",
       "        [-0.03450042, -0.39813107, -0.23370494, -0.06218006,  0.27915868,\n",
       "         -0.09502216, -0.39092863, -0.4534038 ,  0.4206866 , -0.36611205],\n",
       "        [-0.23561552,  0.13641393,  0.5306019 , -0.1363112 , -0.6209317 ,\n",
       "         -0.1086948 , -0.04420102,  0.3257237 , -0.5932869 ,  0.00980924],\n",
       "        [-0.02885425,  0.3180489 , -0.11641473,  0.30534515, -0.26856256,\n",
       "          0.30428317,  0.36721116,  0.53176856, -0.60854423,  0.3845285 ],\n",
       "        [-0.42961988,  0.6105862 , -0.12674482, -0.5190278 , -0.52321523,\n",
       "         -0.5608734 , -0.15569109, -0.41698852,  0.06395306, -0.5191141 ],\n",
       "        [-0.4224332 , -0.27163118, -0.31948084,  0.45928007,  0.49461636,\n",
       "          0.03098603, -0.50372696, -0.19695836,  0.13303715, -0.59088624],\n",
       "        [-0.25692153, -0.26191843, -0.2728497 , -0.43679836,  0.26977128,\n",
       "         -0.11636903,  0.09864499,  0.07884282, -0.24732505,  0.10312327]],\n",
       "       dtype=float32),\n",
       " array([[ 0.1888581 , -0.3652405 , -0.05167288,  0.18925604, -0.01156177,\n",
       "          0.7002544 ,  0.22440153, -0.11430312,  0.26250917,  0.3156405 ],\n",
       "        [ 0.52666396, -0.6477067 ,  0.39507318,  0.01780745, -0.05415613,\n",
       "         -0.17809333, -0.4791479 , -0.03263565,  0.14387031, -0.15935837],\n",
       "        [-0.27026814,  0.2068557 ,  0.62368727,  0.07435026, -0.14698754,\n",
       "          0.04384604, -0.12075424, -0.16440968,  0.5148943 ,  0.3725495 ],\n",
       "        [ 0.16163824,  0.04737344, -0.2937918 ,  0.05023136, -0.10299358,\n",
       "         -0.381223  ,  0.26484555,  0.70127136,  0.4182881 ,  0.23814143],\n",
       "        [-0.20535047,  0.10588025, -0.42213264,  0.5502823 , -0.18198577,\n",
       "         -0.09107298, -0.54051846, -0.03774169, -0.1220807 ,  0.30341473],\n",
       "        [ 0.3261027 ,  0.10296973, -0.21945566, -0.33375338, -0.55463845,\n",
       "         -0.22224346,  0.0706996 , -0.5939911 ,  0.1185273 ,  0.22883023],\n",
       "        [-0.30692592, -0.31082356,  0.00201326, -0.57312673, -0.45356637,\n",
       "          0.3703399 , -0.36716783,  0.25776094, -0.02001341,  0.02682231],\n",
       "        [-0.05029348,  0.23192711, -0.46087185,  0.02141866,  0.2557243 ,\n",
       "          0.07458527, -0.21023642, -0.06967226,  0.6373758 , -0.5303877 ],\n",
       "        [-0.3423109 , -0.17714322, -0.02832579,  0.40787026, -0.46899673,\n",
       "         -0.10660717,  0.37520602, -0.03685993,  0.0203301 , -0.46609583],\n",
       "        [ 0.53491384,  0.41366234,  0.25349128,  0.14815678, -0.50930816,\n",
       "          0.46313295, -0.09401836,  0.1519376 , -0.17168044, -0.13573517]],\n",
       "       dtype=float32),\n",
       " array([ 0.00728045,  0.00074621, -0.00031199, -0.01541864, -0.04306459,\n",
       "         0.01634318,  0.04388991, -0.01635366, -0.01272632,  0.01579256],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>   3\n",
      "<class 'numpy.ndarray'>   10\n",
      "<class 'numpy.float32'>   0.004984821192920208\n"
     ]
    }
   ],
   "source": [
    "x = model.layers[0].get_weights()[0].tolist()\n",
    "print(f'{type(model.layers[1].get_weights())}   {len(model.layers[1].get_weights())}')\n",
    "print(f'{type(model.layers[1].get_weights()[2])}   {len(model.layers[1].get_weights()[2])}')\n",
    "print(f'{type(model.layers[1].get_weights()[2][0])}   {model.layers[1].get_weights()[2][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_weights(model, filename):\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        w = []\n",
    "        [w.append(x.tolist()) for x in layer.get_weights()]\n",
    "        weights.append(w)\n",
    "        \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.0276722 , -0.03214636,  0.04599688,  0.04878035, -0.03438361,\n",
       "         -0.02427794,  0.02166354,  0.04073646, -0.02021405,  0.00705358],\n",
       "        [-0.01297383, -0.03877324,  0.0131318 ,  0.01682233, -0.00295651,\n",
       "         -0.04748244,  0.0160586 , -0.03909265, -0.03347943,  0.00540604],\n",
       "        [-0.02497681,  0.01409762, -0.01605679,  0.00254904,  0.02868929,\n",
       "         -0.04852531, -0.01363418, -0.04606549,  0.04233431, -0.0106622 ],\n",
       "        [ 0.01290948,  0.04114536,  0.003541  , -0.00683626,  0.02310259,\n",
       "         -0.00526153, -0.01384484, -0.0106406 , -0.03317811, -0.02855043]],\n",
       "       dtype=float32),\n",
       " array([[-0.2904532 ,  0.33570272,  0.53752184, -0.50730807, -0.40321356,\n",
       "         -0.36556786, -0.31149718,  0.18843633, -0.03316957, -0.3541723 ],\n",
       "        [ 0.18768597, -0.4660169 ,  0.2519024 ,  0.13419265, -0.28947142,\n",
       "          0.29118562,  0.16654259, -0.32439777, -0.52050495,  0.16858327],\n",
       "        [-0.15505055, -0.34722754,  0.12369317, -0.08359307, -0.08467448,\n",
       "         -0.27287757,  0.2735458 ,  0.03212631, -0.24733746, -0.10675284],\n",
       "        [ 0.05696857,  0.03726375,  0.04521203, -0.09963179, -0.06204173,\n",
       "         -0.14465332,  0.12448817,  0.278562  , -0.4905349 ,  0.54244196],\n",
       "        [-0.12114969,  0.05325699,  0.41660762, -0.42600417,  0.12278622,\n",
       "          0.5356698 , -0.22308034,  0.14738387, -0.20885521, -0.52474225],\n",
       "        [-0.07606092, -0.35726488, -0.15647015, -0.4512686 ,  0.44027895,\n",
       "          0.4790429 ,  0.37916738, -0.43667144,  0.5320662 , -0.34342864],\n",
       "        [-0.5440382 ,  0.46523273,  0.14597917,  0.00236952, -0.10931394,\n",
       "         -0.12682882,  0.46796465,  0.3020534 ,  0.20818281,  0.27059478],\n",
       "        [ 0.27342027, -0.11647508,  0.39838052, -0.5091925 ,  0.22171634,\n",
       "         -0.07084998,  0.16631275, -0.30171397, -0.467089  ,  0.08486474],\n",
       "        [-0.3665935 ,  0.43045002,  0.03565669,  0.17765743,  0.29302353,\n",
       "         -0.4817798 ,  0.10150677, -0.16467872,  0.51325846, -0.51867217],\n",
       "        [-0.32601953,  0.20028085,  0.05130643,  0.3745075 ,  0.52547896,\n",
       "          0.51120734,  0.12794954,  0.06260312,  0.45094883, -0.00102174]],\n",
       "       dtype=float32),\n",
       " array([[-0.3973571 ,  0.25024524, -0.44272763,  0.0157309 ,  0.04986115,\n",
       "          0.3319549 , -0.32771033,  0.4099799 , -0.20499858, -0.39121222],\n",
       "        [ 0.4743033 , -0.19221917, -0.2356038 ,  0.48949063,  0.26888165,\n",
       "          0.13251917, -0.06396724,  0.46559703,  0.29191577,  0.21686864],\n",
       "        [-0.30818552, -0.31325674,  0.04157778, -0.2792555 ,  0.47969806,\n",
       "         -0.3980546 ,  0.12983409,  0.41547185, -0.2892133 ,  0.2559374 ],\n",
       "        [-0.16089751,  0.10295066,  0.15412578, -0.2324269 , -0.12272598,\n",
       "          0.659129  ,  0.4642928 ,  0.2335837 ,  0.0912683 ,  0.39719537],\n",
       "        [-0.06091277, -0.36204508,  0.1832598 ,  0.4624363 , -0.04536277,\n",
       "          0.3088985 , -0.10264744, -0.19561896, -0.67387116,  0.1319089 ],\n",
       "        [-0.35917723,  0.2585123 ,  0.19591361,  0.45233467, -0.49523076,\n",
       "         -0.37665677, -0.00984738,  0.3236736 ,  0.07484755,  0.25216416],\n",
       "        [ 0.39195862,  0.50103587,  0.51847196, -0.1710887 ,  0.15697078,\n",
       "          0.03504347, -0.37022635,  0.23351792, -0.272063  ,  0.07591237],\n",
       "        [-0.07212251,  0.56802523, -0.1751146 ,  0.3464271 ,  0.4772158 ,\n",
       "         -0.07811101,  0.4280267 , -0.27933553, -0.14707728,  0.06903357],\n",
       "        [ 0.40849045,  0.10764077, -0.52505064, -0.2176539 , -0.4143565 ,\n",
       "         -0.17815816,  0.18648003,  0.09217788, -0.45401275,  0.21369812],\n",
       "        [-0.19457872,  0.09229854, -0.27545768, -0.11417156,  0.09309985,\n",
       "          0.05466146, -0.53889394, -0.3222895 ,  0.1389407 ,  0.662926  ]],\n",
       "       dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.2089901 ,  0.12994248,  0.61883426],\n",
       "        [ 0.46885085, -0.04176182, -0.30750707],\n",
       "        [ 0.5318521 , -0.20474085,  0.30350876],\n",
       "        [-0.6518108 ,  0.03868347,  0.04764098],\n",
       "        [-0.12846917, -0.58392835,  0.48894024],\n",
       "        [-0.3868477 ,  0.5340718 , -0.11070925],\n",
       "        [-0.36208907, -0.11034137,  0.46394026],\n",
       "        [-0.01476455,  0.2712791 , -0.31176844],\n",
       "        [-0.08676064, -0.15132928,  0.3420304 ],\n",
       "        [-0.37916628,  0.04078799,  0.5398352 ]], dtype=float32),\n",
       " array([0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.layers.core.embedding.Embedding object at 0x000002870F948190>, <keras.layers.rnn.simple_rnn.SimpleRNN object at 0x000002871A8EEC10>, <keras.layers.core.dense.Dense object at 0x000002871B605210>]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'weights.txt'\n",
    "#save_weights(model, filename)\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(model.layers, f)\n",
    "#print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.embedding.Embedding at 0x2870f6a7650>,\n",
       " <keras.layers.rnn.simple_rnn.SimpleRNN at 0x2870f6c66d0>,\n",
       " <keras.layers.core.dense.Dense at 0x2870f694d90>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(filename, 'rb') as f:\n",
    "    rer = pickle.load(f)\n",
    "rer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rer = [np.random.randn(10,3), np.ones(3)]\n",
    "len(rer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n",
      "[0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.array([[1]*len(model.layers[2].get_weights()[0][0])]*len(model.layers[2].get_weights()[0]))\n",
    "print(np.asarray(x).shape)\n",
    "y = np.array([0]*len(model.layers[2].get_weights()[1]))\n",
    "print(y)\n",
    "np.asarray(y).shape\n",
    "model.layers[2].set_weights([x, y])\n",
    "\n",
    "model.layers[2].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10   3\n",
      "3   -0.004997094161808491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.04087391,  0.01574843, -0.00389519, -0.04208458, -0.01874881,\n",
       "          0.05217679, -0.03702322,  0.02232463,  0.01298681, -0.03275087],\n",
       "        [ 0.02268491,  0.03186835, -0.02343684, -0.03106025,  0.03337424,\n",
       "          0.00162261, -0.01158115,  0.01741067, -0.00934891,  0.01602577],\n",
       "        [ 0.02666692,  0.04172854, -0.04967573,  0.01773047,  0.00267124,\n",
       "          0.05301905,  0.01654921,  0.01729628,  0.02256451, -0.00616259],\n",
       "        [-0.03813278,  0.00434956, -0.02140179,  0.02989013,  0.02575764,\n",
       "          0.00664869,  0.01316514, -0.00521238, -0.00237513, -0.02024618]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model.layers[2].get_weights()))\n",
    "print(f'{len(model.layers[2].get_weights()[0])}   {len(model.layers[2].get_weights()[0][0])}')\n",
    "print(f'{len(model.layers[2].get_weights()[1])}   {model.layers[2].get_weights()[1][0]}')\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10   10\n",
      "10   10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers[1].get_weights()))\n",
    "print(f'{len(model.layers[1].get_weights()[0])}   {len(model.layers[1].get_weights()[0][0])}')\n",
    "print(f'{len(model.layers[1].get_weights()[1])}   {len(model.layers[1].get_weights()[1][0])}')\n",
    "print(len(model.layers[1].get_weights()[2]))\n",
    "#model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 10)          40        \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, None, 10)          210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.pop()\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0]*10]*10)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "4\n",
      "10\n",
      "\n",
      "\n",
      "2\n",
      "10\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gael\\AppData\\Local\\Temp\\ipykernel_7376\\641384744.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.asarray(weights).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = []\n",
    "weights.append(np.array([[[0.0]*10]*4]))\n",
    "weights.append([np.array([[0.0]*10]*10), np.array([[0.0]*10]*10), np.array([0.0]*10)])\n",
    "weights.append([np.array([[0.0]*3]*10), np.array([0.0]*3)])\n",
    "print(len(weights))\n",
    "print(len(weights[0]))\n",
    "print(len(weights[0][0]))\n",
    "print(len(weights[0][0][0]))\n",
    "print('\\n')\n",
    "print(len(weights[2]))\n",
    "print(len(weights[2][0]))\n",
    "print(len(weights[2][0][0]))\n",
    "print(len(weights[2][1]))\n",
    "np.asarray(weights).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_weights(model, filename, return_states=True):\n",
    "    with open(filename, 'r') as f:\n",
    "        weights = json.load(f)\n",
    "    \n",
    "    model.layers[0].set_weights([np.array(weights[0][0])])\n",
    "    model.layers[1].set_weights([np.array(weights[1][0]), np.array(weights[1][1]), np.array(weights[1][2])])\n",
    "\n",
    "    if not return_states:\n",
    "       model.layers[2].set_weights([np.array(weights[2][0]), np.array(weights[2][1])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, None, 10)          40        \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, None, 10)          210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(4, embedding_vector_length))\n",
    "model1.add(SimpleRNN(10, return_sequences=True))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', ignore_class_accuracy(2)])\n",
    "#load_weights(model1, 'weights.txt')\n",
    "model1.layers[0].set_weights(weights[0])\n",
    "model1.layers[1].set_weights(weights[1])\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 152ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 4, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['abbb', 'bbbz', 'abab', 'aaaa', 'abba']\n",
    "x_predict = np.array([tokenization(x) for x in data])\n",
    "x_predict = tf.convert_to_tensor(x_predict)\n",
    "ter = model1.predict(x_predict)\n",
    "ter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 400ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 100, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['abbb'+'ab'*48, 'bbba'+'ab'*48, 'abab'+'ab'*48, 'aaaa'+'ab'*48]\n",
    "x_predict = np.array([tokenization(x) for x in data])\n",
    "states = model1.predict(x_predict)\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2459124702.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    def load_weights((model, filepath, only_state=False)):\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def Tagger(n_tokens = 4, embedding_vector_length = 10, hidden_dim = 10, n_labels = 3, return_states = False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(n_tokens, embedding_vector_length))\n",
    "    model.add(SimpleRNN(hidden_dim, return_sequences=True))\n",
    "    if not return_states:\n",
    "        model.add(Dense(n_labels, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def load_weights(model, weights, return_states=False):\n",
    "    if return_states:\n",
    "        model.set_weights()\n",
    "\n",
    "def save_weights(model, filepath):\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(model.layers, f)\n",
    "\n",
    "def load_weights(model, filepath, only_state=False)):\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(model.layers, f)\n",
    "    if not only_state:\n",
    "        model.layers[0].set_weights([model.layers[0].get_weights()[0]])\n",
    "        model.layers[1].set_weights([model.layers[1].get_weights()[0], model.layers[1].get_weights()[1], model.layers[1].get_weights()[2]])\n",
    "\n",
    "model3 = Tagger()\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', ignore_class_accuracy(2)])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['ba', 'b', 'a', 'baa', 'a', 'baaa', 'aa', 'b', 'abaa', 'abb', 'bb']\n",
    "labels = ['11', '1', '1', '110', '1', '1100', '10', '1', '1010', '101', '11']\n",
    "corpus_ = [ \"e\"+x+\"z\"*(4-len(x)) for x in corpus]\n",
    "labels_ = [\"0\"+x+\"2\"*(4-len(x)) for x in labels]\n",
    "states = [get_hidden_state(x) for x in corpus_]\n",
    "mask = [x!=\"z\" for x in corpus_]\n",
    "\n",
    "\n",
    "#print(corpus_)\n",
    "#print(states)\n",
    "\n",
    "x_train = tf.convert_to_tensor([tokenization(x) for x in corpus_])\n",
    "train_sents = [tokenization(x) for x in corpus]\n",
    "y_train = tf.convert_to_tensor([class_mapping(x) for x in labels_])\n",
    "mask = tf.convert_to_tensor([masking(x) for x in corpus_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 3, 3],\n",
       "       [2, 1, 3, 3, 3],\n",
       "       [2, 0, 3, 3, 3],\n",
       "       [2, 1, 0, 0, 3],\n",
       "       [2, 0, 3, 3, 3],\n",
       "       [2, 1, 0, 0, 0],\n",
       "       [2, 0, 0, 3, 3],\n",
       "       [2, 1, 3, 3, 3],\n",
       "       [2, 0, 1, 0, 0],\n",
       "       [2, 0, 1, 1, 3],\n",
       "       [2, 1, 1, 3, 3]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
