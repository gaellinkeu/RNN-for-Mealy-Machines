# -*- coding: utf-8 -*-
"""Rnn formealy machines.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eymx8AFmtCwA0RfoNKuMosXV2KxWVvZ0

#Entrainement d'unn réseau de neurone sur un langage régulier

Nous allons prendre un datasets composés de mots génerés aléatoirement puis on a obtenu les outputs à partir d'une machine de Mealy


*   Inputs de longueur aléatoire entre 2 et 100
*   Une Machine de Mealy générées aléatoirement
*
"""

import os
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
tf.config.run_functions_eagerly(True)
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Embedding
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from utils import *
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint

from model import Tagger

if __name__ == '__main__' :

  """1- Lecture du fichier"""

  inputs, outputs, max_length = get_data('dataset2.txt')
    
  print(f'The size of inputs set is {len(inputs)}\n')
  print(inputs[:5])

  print(f'\nThe size of outputs set is {len(outputs)}\n')
  print(outputs[:5])
  print(f'The max length of inputs {max_length}')



  """2- Prétraitement des données"""

  # Complétion à la taille du mot le plus longs

  train = []
  labels = []
  masks = []

  # De vecteurs de mots à matrixes de charactères
  for i in range(len(inputs)):
    word = []
    lbl = []
    mask = []
    for j in range(len(inputs[i])):
        word.append(inputs[i][j])
        lbl.append(outputs[i][j])
        mask.append(True)
    train.append(word)
    labels.append(lbl)
    masks.append(mask)

  for i in range(len(train)):
    if len(train[i]) < max_length:
      l = len(train[i])
      train[i] += ["z" for _ in range(max_length-l)] #Completion avec les "z"
      labels[i] += ["2" for _ in range(max_length-l)] # Label de "z" à 2 
      masks[i] += [False for _ in range(max_length-l)]
      #train[i][0] += "z"*(max-l)
      #labels[i][0] += "0"*(max-l)
  #print([len(x) for x in label])
  print(train[0][50:])
  print(labels[0][50:])
  print(masks[0][50:])

  # Tokenisation des mots et leurs labels
  for i in range(len(train)):
    for j in range(len(train[i])):
      if train[i][j][0] == "a":
        train[i][j] = 1 # Tokenisation de a à 1
      elif train[i][j][0] == "b": 
        train[i][j] = 2 # Tokenisation de b à 2
      else:
        train[i][j] = 0 # Tokenisation de z à 0
      
      labels[i][j] = int(labels[i][j][0])
  #train[1]
  #train = [[x] for x in train]
  print(len(train[1]))
  print(train[1][:50])
  print(labels[1][:50])

  y = []
  for i in range(len(labels)):
    y.append([])
    for j in labels[i]:
      if j == 0:
        y[i].append([1,0,0])
      elif j == 1:
        y[i].append([0,1,0])
      else:
        y[i].append([0,0,1])

  print(y[0][:7])


  train_ = np.array(train)
  labels_ = np.array(y)

  print(train_.shape)
  print(labels_.shape)



  # Division en jeu de test et jeu d'entrainement
  X_train, X_test, y_train, y_test = train_test_split(train_, labels_, test_size=0.2, random_state = 42)
  
  # Masquage de jeu d'entrainement et de test
  train_mask = X_train[X_train != '0']
  test_mask = X_test[X_test != '0']


  """3- Définition du modèle"""

  n_epochs = 15
  batch_size = 50
  embedding_vector_length = 10


  """4- Entrainement du modèle"""

  #Modèle pour une classification avec 3 classes ( pad characters have their own class)
  model = Sequential()
  model.add(Embedding(3, embedding_vector_length, input_length=max_length))
  model.add(LSTM(100, return_sequences=True))
  model.add(Dense(3, activation='softmax'))
  # Compile model
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', ignore_class_accuracy(2)])
  # checkpoint
  filepath = "weigths/model_weights.hd5"
  print(model.summary())

  # Fit the model
  history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, callbacks=callbacks_list, verbose=1)

  version_name = None
  model_dir = os.path.join("weigths", version_name)
  if not os.path.exists(model_dir):
      os.makedirs(model_dir)
  filepath = "weigths/model_weights.h5"


  model.save_weights(filepath)

  print(history.history.keys())
  acc = [x*100 for x in history.history['ignore_accuracy']]
  val_acc = [x*100 for x in history.history['val_ignore_accuracy']]
  epochs = range(n_epochs)
  plt.plot(epochs, acc, 'g', label='Trainning Accuracy')
  plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('ACC')
  plt.legend()
  plt.show()

  """6- Application sur le jeu de test"""

  # Final evaluation of the model
  scores = model.evaluate(X_test, y_test, verbose=0)
  predictions = model.predict(X_test)
  print("Accuracy: %.2f%%" % (scores[1]*100))


  print("La matrice de confusion est")
  conf_matrix = confusion_matrix(y_test.argmax(axis=-1).flatten(), predictions.argmax(axis=-1).flatten())
  print(conf_matrix)
  prf = precision_recall_fscore_support(y_test.argmax(axis=-1).flatten(), predictions.argmax(axis=-1).flatten())
  for i in range(3):
    print(f"La classe {i}: la précision: {prf[i][0]}, le rappel: {prf[i][1]}, la f-mesure: {prf[i][2]}\n")
    
