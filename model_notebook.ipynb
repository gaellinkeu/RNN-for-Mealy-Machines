{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# Accuracy ne prenant pas en compte les charactères complétés\n",
    "\n",
    "# Remove this when the cosineSimilarity will be added\n",
    "def cosineSimilarity(h1, h2):\n",
    "    return 2.3\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=2):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "# Fonction qui imitte le comportement du réseau de neurone\n",
    "def get_hidden_state (word):\n",
    "    prec = 0.005\n",
    "    output = []\n",
    "    \n",
    "    for i, char in enumerate(word):\n",
    "        if char == \"a\":\n",
    "            prec = 0.02*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        elif char == \"b\":\n",
    "            prec = 0.03*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        elif char == \"e\":\n",
    "            prec = 0.005*(i+1) + prec\n",
    "            output.append(prec)\n",
    "        else:\n",
    "            prec = 0.05*(i+1) + prec\n",
    "            output.append(prec)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_data(filepath):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print(lines[:3])\n",
    "\n",
    "\n",
    "    max_length = 0\n",
    "\n",
    "    for line in lines:\n",
    "        res = \"\"\n",
    "        isInput = True\n",
    "        for symbol in line:\n",
    "            if symbol in [',', '\\n']:\n",
    "                if isInput:\n",
    "                    inputs.append(res)\n",
    "                    max_length = len(res) if len(res) > max_length else max_length\n",
    "                    res = \"\"\n",
    "                    isInput = not isInput\n",
    "                    continue\n",
    "                else:\n",
    "                    outputs.append(res)\n",
    "            res += symbol\n",
    "        #print(line)\n",
    "    return inputs, outputs, max_length\n",
    "\n",
    "def merging_checking(st1, st2, k):\n",
    "    similarity = False\n",
    "    consistency = False\n",
    "\n",
    "    # for the similarity, we will merge state1 to state 2 if\n",
    "    # If every input of state1 are in the input set of state2 and\n",
    "    # If for each input state1 input set, we get the same output\n",
    "    # from state1 and state2\n",
    "\n",
    "    if set(st1._outTr.keys()) in set(st2._outTr.keys()):\n",
    "        for char in list(st1._outTr.keys()):\n",
    "            if set(st1._outTr[char].keys()) == set(st1._outTr[char].keys()):\n",
    "                similarity = True\n",
    "\n",
    "    # compute the cosine similarity of the two hidden state value\n",
    "    # If it's greater than k, the consistency constraint in respected\n",
    "    if cosineSimilarity(st1.hidden_state, st2.hidden_state) > k:\n",
    "        consistency = True\n",
    "\n",
    "    return similarity and consistency\n",
    "\n",
    "def class_mapping(label, numb_class = 3):\n",
    "    y_train = []\n",
    "    for x in label:\n",
    "        assert int(x) < numb_class\n",
    "        y_train.append([int(i==int(x)) for i in range(numb_class)])\n",
    "        \n",
    "    return y_train\n",
    "\n",
    "def tokenization(word, num_token = 4):\n",
    "    x_train = []\n",
    "    for x in word:\n",
    "        if x == 'a':\n",
    "            x_train.append(1)\n",
    "        elif x == 'b':\n",
    "            x_train.append(2)\n",
    "        elif x == 'e':\n",
    "            x_train.append(3)\n",
    "        else:\n",
    "            x_train.append(0)\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "def masking(word, pad_char = 'z'):\n",
    "    return [x!=pad_char for x in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "class Tagger(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, n_tokens = 3, embed_dim = 10, max_length = 99, rnn_dim = 10, n_labels=3):\n",
    "    super().__init__()\n",
    "    self.embedding = Embedding(n_tokens, embed_dim, input_length=max_length, mask_zero=True)\n",
    "    self.rnn = SimpleRNN(rnn_dim, return_sequences=True)\n",
    "    self.outputs = Dense(n_labels, activation='softmax')\n",
    "\n",
    "  def call(self, token_ids, labels, mask, training = True):\n",
    "    embeddings = self.embedding(token_ids)\n",
    "    states = self.rnn(embeddings)\n",
    "    logits = self.outputs(states)\n",
    "    loss = CategoricalCrossentropy()(labels, logits)\n",
    "    #predictions = tf.math.argmax(logits, axis=-1)\n",
    "    bool_acc = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(bool_acc, tf.float32))\n",
    "    #acc = ((predictions == labels) * mask).sum().float() / mask.sum()\n",
    "    return {\n",
    "            \"states\": states,\n",
    "            \"predictions\": logits,\n",
    "            \"accuracy\": accuracy.numpy(),\n",
    "            \"loss\": loss,\n",
    "          }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaabbabbbbbbbbbaaabbbbbabbaabbbaa,0101110111111111011111110110111101\\n', 'bbaaaabaaabbaabababbaabbabbaabbbababaababaaaaabbababbbab,11011110111101101011011101101111010101101011111101011101\\n', 'bbbabbabababaaaabbbbbaabaababaababbbbbbabbbbaaabbaaabababaaaab,11101101010101111111101101101011011111101111011110111010101111\\n']\n",
      "(10000, 100)\n",
      "[3 1 2 1 1 2 2 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 1 2 2 1 1 2 2 2 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from model import Tagger \n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    #max_length = 4\n",
    "    #corpus = ['ba', 'b', 'a', 'baa', 'a', 'baaa', 'aa', 'b', 'abaa', 'abb', 'bb']\n",
    "    #labels = ['11', '1', '1', '110', '1', '1100', '10', '1', '1010', '101', '11']\n",
    "    corpus, labels, max_length = get_data('dataset2.txt')\n",
    "    corpus_ = [\"e\"+x+\"z\"*(max_length-len(x)) for x in corpus]\n",
    "    labels_ = [\"0\"+x+\"2\"*(max_length - len(x)) for x in labels]\n",
    "    states = []\n",
    "\n",
    "    n_epochs = 10\n",
    "    batch_size = 10\n",
    "\n",
    "    x_train = np.array([tokenization(x) for x in corpus_])\n",
    "    y_train = np.array([class_mapping(x) for x in labels_])\n",
    "    mask = np.array([masking(x) for x in corpus_])\n",
    "\n",
    "    version_name = '01'\n",
    "    model_dir = os.path.join(\"weigths\", version_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    filepath = \"weigths/model_weights.h5\"\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "    trained_model = Tagger(4, 10, max_length+1, 10)\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    trained_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #trained_model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
    "    \n",
    "    train_results = trained_model(x_train, y_train, mask)\n",
    "\n",
    "    \"\"\"for epoch in range(5):\n",
    "        for batch_idx in trange(0, len(x_train) - 2, 2):\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_tokens = x_train[batch_idx:batch_idx + 2]\n",
    "                batch_labels = y_train[batch_idx:batch_idx + 2]\n",
    "                batch_mask = mask[batch_idx:batch_idx + 2]\n",
    "                train_results = trained_model(batch_tokens, batch_labels, batch_mask)\n",
    "                \n",
    "                loss = train_results['loss']\n",
    "            grads = tape.gradient(loss, trained_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trained_model.trainable_variables))\n",
    "        #trained_model.save_weights(filepath)\n",
    "        train_results = trained_model(x_train, y_train, mask)\n",
    "    \n",
    "        train_preds = train_results[\"predictions\"]\n",
    "        #print('\\n\\n\\n Les prédictions sont: \\n\\n')\n",
    "        #print(train_preds)\n",
    "        \n",
    "\n",
    "        print(f'\\n\\n The accuracy: {train_results[\"accuracy\"]}')\n",
    "\n",
    "    #trained_model.save_weights(\"weights.\")\n",
    "\n",
    "    representations = train_results[\"states\"][:5]\n",
    "    print('\\n\\n\\n Les étatss sont: \\n\\n')\n",
    "    print(representations)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor([[[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]],\n\u001b[0;32m      4\u001b[0m                                 [[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]],\n\u001b[0;32m      5\u001b[0m                                 [[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]],\n\u001b[0;32m      6\u001b[0m                                 [[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]],\n\u001b[0;32m      7\u001b[0m                                 [[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]]])\n\u001b[1;32m----> 9\u001b[0m logits \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor([[[\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0.1\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m]],\n\u001b[0;32m     10\u001b[0m                                 [[\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0.1\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m,\u001b[39m0\u001b[39;49m]],\n\u001b[0;32m     11\u001b[0m                                 [[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m]],\n\u001b[0;32m     12\u001b[0m                                 [[\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0.1\u001b[39;49m,\u001b[39m9\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m]],\n\u001b[0;32m     13\u001b[0m                                 [[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m],[\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m,\u001b[39m0\u001b[39;49m],[\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m,\u001b[39m0.9\u001b[39;49m]]])\n\u001b[0;32m     15\u001b[0m bool_acc \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mequal(tf\u001b[39m.\u001b[39margmax(logits, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), tf\u001b[39m.\u001b[39margmax(labels, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     16\u001b[0m accuracy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mcast(bool_acc, tf\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "labels = tf.convert_to_tensor([[[1,0,0],[1,0,0],[0,0,1]],\n",
    "                                [[0,1,0],[1,0,0],[0,1,0]],\n",
    "                                [[0,0,1],[0,0,1],[0,0,1]],\n",
    "                                [[0,1,0],[1,0,0],[0,0,1]],\n",
    "                                [[0,0,1],[0,1,0],[0,0,1]]])\n",
    "\n",
    "logits = tf.convert_to_tensor([[[0.1,0.9,0],[0.1,9,0,0],[0,0.1,0.9]],\n",
    "                                [[0.1,0.9,0],[0.1,9,0,0],[0.1,0.9,0]],\n",
    "                                [[0,0.1,0.9],[0,0.1,0.9],[0,0.1,0.9]],\n",
    "                                [[0.1,0.9,0],[0.1,9,0,0],[0,0.1,0.9]],\n",
    "                                [[0,0.1,0.9],[0.1,0.9,0],[0,0.1,0.9]]])\n",
    "\n",
    "bool_acc = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))\n",
    "accuracy = tf.reduce_mean(tf.cast(bool_acc, tf.float32))\n",
    "\n",
    "print(accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baby come over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create an optimizer.\n",
    "opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)\n",
    "var1, var2 = tf.Variable(1.0), tf.Variable(2.0)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = 3 * var1 * var1 + 2 * var2 * var2\n",
    "grads = tape.gradient(loss, [var1, var2])\n",
    "\n",
    "# Process the gradients.\n",
    "grads[0] = grads[0] + 1\n",
    "\n",
    "# Ask the optimizer to apply the gradients on variables.\n",
    "opt.apply_gradients(zip(grads, [var1, var2]))\n",
    "print(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_all_prefix(mealy, dataset, labels):\n",
    "    # A bunch of code on how to determine if a label correctly corresponds\n",
    "    # to the output of the mealy machine\n",
    "    scores = 0\n",
    "    total = 0\n",
    "    for i in range(len(dataset)):\n",
    "        output = mealy.return_output(dataset[i])\n",
    "        print(output)\n",
    "        score = [labels[i][j] == output[j] for j in range(len(output))]\n",
    "        scores += score.count(True)\n",
    "        total += len(output)\n",
    "\n",
    "    return scores/total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 'a', '0', 3], [1, 'b', '0', 3], [0, 'b', '0', 3]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "a = ['eaaab', 'ebabb', 'eaac', 'eaab']\n",
    "b = ['00001', '01011', '0001', '0010']\n",
    "arcs = [[2,'a', '0', 3],[1,'b', '0', 3],[2,'a', '0', 3],[0,'b', '0', 3],[1,'b', '0', 3],]\n",
    "\n",
    "class Mealy(object):\n",
    "\n",
    "    def __init__(self, id, arcs):\n",
    "        self.id = id\n",
    "        self.transitions = arcs\n",
    "\n",
    "    def return_output(self, word):\n",
    "        # word must always start with 'e' representing the bos\n",
    "        output = ''\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in {'e', 'a'}:\n",
    "                output += f'{0}'\n",
    "            else:\n",
    "                output += f'{1}'\n",
    "        return output\n",
    "    \n",
    "    def removeDuplicateTransitions(self):\n",
    "        add = True\n",
    "        transitions = [self.transitions[0]]\n",
    "        for x in self.transitions:\n",
    "            for y in transitions:\n",
    "                if x == y:\n",
    "                    add = False\n",
    "            if add:\n",
    "                transitions.append(x)\n",
    "            add = True\n",
    "        self.transitions = deepcopy(transitions)\n",
    "\n",
    "m = Mealy(0, arcs)\n",
    "m.removeDuplicateTransitions()\n",
    "print(m.transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of states is 11\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "The amount of arcs is 10\n",
      "\n",
      "\n",
      "Different transitions of the Tree: \n",
      "0 --> b/1 --> 1\n",
      "1 --> a/1 --> 2\n",
      "2 --> a/0 --> 4\n",
      "4 --> a/0 --> 5\n",
      "0 --> a/1 --> 3\n",
      "3 --> b/0 --> 7\n",
      "7 --> b/1 --> 10\n",
      "7 --> a/1 --> 8\n",
      "8 --> a/0 --> 9\n",
      "3 --> a/0 --> 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "class Mealy(object):\n",
    "\n",
    "    def __init__(self, id, root, nodes, arcs):\n",
    "        # nodes = [0,1,2,...]\n",
    "        # arcs = [(0,a,1,1), ...]\n",
    "        self.id = id\n",
    "        self.root =  root\n",
    "        self.nodes = nodes\n",
    "        self.transitions = [list(x) for x in arcs]\n",
    "    \n",
    "    def output(self, input_state, input_char):\n",
    "        for x in self.transitions:\n",
    "            if x[0] == input_state and x[1] ==  input_char:\n",
    "                return x[2], x[3]\n",
    "        return None\n",
    "\n",
    "    def getInpOut(self, node):\n",
    "        inp_out = []\n",
    "        for x in self.transitions:\n",
    "            if x[0] == node:\n",
    "                inp_out.append([x[1],x[2]])\n",
    "        \n",
    "        return inp_out\n",
    "\n",
    "    # get the output of the machine given a word\n",
    "    def return_output(self, word):\n",
    "        # we consider that the word comes without the bos sign\n",
    "        output = ''\n",
    "        idx = self.root\n",
    "        for i in range(len(word)):\n",
    "           if self.output(idx, word[i]) == None:\n",
    "            print(f'There\\'s no transitions from {idx} with {word[i]}')\n",
    "            break\n",
    "           output += self.output(idx, word[i])[0]\n",
    "           idx = self.output(idx, word[i])[1]\n",
    "        return output\n",
    "    \n",
    "    # get the trace of the machine given a word\n",
    "    def return_states(self, word):\n",
    "        \n",
    "        # we consider that the word comes without the bos sign\n",
    "        # for a word abba we have [0,1,2,3,4] for example\n",
    "        idx = [self.root]\n",
    "        for i in range(len(word)):\n",
    "           if self.output(idx[i], word[i]) == None:\n",
    "            print(f'There\\'s no transitions from {idx[i]} with {word[i]}')\n",
    "            break\n",
    "           idx.append(self.output(idx[i], word[i])[1])\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "    def print(self):\n",
    "        print(f'The amount of states is {len(self.nodes)}')\n",
    "        print(self.nodes)\n",
    "        #print(\"Different states of the Tree: \")\n",
    "        #for i in self.nodes:\n",
    "        #    print(f'ID: {i}\\tHidden value: {0}')\n",
    "\n",
    "        print(f'\\nThe amount of arcs is {len(self.transitions)}\\n')\n",
    "        print(\"\\nDifferent transitions of the Tree: \")\n",
    "        for transition in self.transitions:\n",
    "            print(f'{transition[0]} --> {transition[1]}/{transition[2]} --> {transition[3]}')\n",
    "\n",
    "    def removeDuplicate(self):\n",
    "        add = True\n",
    "        states = []\n",
    "        for x in self.nodes:\n",
    "            if x not in states:\n",
    "                states.append(x)\n",
    "        \n",
    "        self.nodes = deepcopy(states)\n",
    "\n",
    "        transitions = [self.transitions[0]]\n",
    "        for x in self.transitions:\n",
    "            for y in transitions:\n",
    "                if x == y:\n",
    "                    add = False\n",
    "            if add:\n",
    "                transitions.append(x)\n",
    "            add = True\n",
    "        self.transitions = deepcopy(transitions)\n",
    "\n",
    "        nodes = []\n",
    "        for x in self.transitions:\n",
    "            if x[0] not in nodes:\n",
    "                nodes.append(x[0])\n",
    "            if x[3] not in nodes:\n",
    "                nodes.append(x[3])\n",
    "\n",
    "        self.nodes = deepcopy(nodes)\n",
    "        \n",
    "        \n",
    "    def merge_states(self, state1, state2):\n",
    "        \n",
    "\n",
    "        self.merging(state1, state2)\n",
    "        \n",
    "        #self.removeDuplicate()\n",
    "        self.print()\n",
    "\n",
    "\n",
    "    def merging(self, state1, state2):\n",
    "        print(f'\\n The two states are {state1} and {state2}\\n')\n",
    "        submerged = False\n",
    "        if state1 == state2:\n",
    "            return 0\n",
    "        if (state1 not in self.nodes or state2 not in self.nodes):\n",
    "            return 1\n",
    "        \n",
    "        for i in range(len(self.transitions)):\n",
    "            for j in range(len(self.transitions)):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "\n",
    "                # merge the children of the two mergable states\n",
    "                if self.transitions[i][0] == state1 and self.transitions[j][0] == state2:\n",
    "                    if self.transitions[i][1:3] == self.transitions[j][1:3]:\n",
    "                        submerged = True\n",
    "                        print(f'\\n The two SUB states are {self.transitions[i][3]} and {self.transitions[j][3]}\\n')\n",
    "                        self.merging(self.transitions[i][3], self.transitions[j][3])\n",
    "\n",
    "        for i in range(len(self.transitions)):\n",
    "            if self.transitions[i][0] == state2:\n",
    "                self.transitions[i][0] = state1\n",
    "            if self.transitions[i][3] == state2:\n",
    "                self.transitions[i][3] = state1\n",
    "        \n",
    "        # If the merged is the root\n",
    "        if self.root == state2:\n",
    "            self.root = state1\n",
    "\n",
    "        # Delete the merged state\n",
    "        \"\"\"if state2 in self.nodes:\n",
    "            self.nodes.remove(state2)\"\"\"\n",
    "\n",
    "        # Remove doble transaction\n",
    "        transitions = []\n",
    "\n",
    "        \"\"\"for x in self.transitions:\n",
    "            add = True\n",
    "            for y in transitions:\n",
    "                if x == y:\n",
    "                    add = False\n",
    "            if add:\n",
    "                transitions.append(x)\n",
    "            \n",
    "        self.transitions = deepcopy(transitions)\"\"\"\n",
    "        #print(self.transitions)\n",
    "        \n",
    "        \n",
    "        #self.print()\n",
    "\n",
    "        return 0\n",
    "\n",
    "nodes = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "arcs = [(0,'b','1',1),\n",
    "        (1,'a','1',2),\n",
    "        (2,'a','0',4),\n",
    "        (4,'a','0',5),\n",
    "        (0,'a','1',3),\n",
    "        (3,'b','0',7),\n",
    "        (7,'b','1',10),\n",
    "        (7,'a','1',8),\n",
    "        (8,'a','0',9),\n",
    "        (3,'a','0',6)]\n",
    "\n",
    "fsm = Mealy(0, 0, nodes, arcs)\n",
    "fsm.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 7, 10]\n",
      "101\n",
      "There's no transitions from 1 with b\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(fsm.return_states('abb'))\n",
    "print(fsm.return_output('abb'))\n",
    "print(fsm.return_states('bbb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The two states are 0 and 1\n",
      "\n",
      "The amount of states is 11\n",
      "\n",
      "\n",
      "The amount of arcs is 10\n",
      "\n",
      "\n",
      "Different transitions of the Tree: \n",
      "0 --> b/1 --> 0\n",
      "0 --> a/1 --> 3\n",
      "3 --> a/0 --> 3\n",
      "3 --> a/0 --> 3\n",
      "0 --> a/1 --> 3\n",
      "3 --> b/0 --> 7\n",
      "7 --> b/1 --> 10\n",
      "7 --> a/1 --> 8\n",
      "8 --> a/0 --> 9\n",
      "3 --> a/0 --> 3\n",
      "\n",
      " The two states are 3 and 6\n",
      "\n",
      "The amount of states is 11\n",
      "\n",
      "\n",
      "The amount of arcs is 10\n",
      "\n",
      "\n",
      "Different transitions of the Tree: \n",
      "0 --> b/1 --> 0\n",
      "0 --> a/1 --> 3\n",
      "3 --> a/0 --> 3\n",
      "3 --> a/0 --> 3\n",
      "0 --> a/1 --> 3\n",
      "3 --> b/0 --> 7\n",
      "7 --> b/1 --> 10\n",
      "7 --> a/1 --> 8\n",
      "8 --> a/0 --> 9\n",
      "3 --> a/0 --> 3\n",
      "\n",
      " The two states are 0 and 7\n",
      "\n",
      "\n",
      " The two SUB states are 0 and 10\n",
      "\n",
      "\n",
      " The two states are 0 and 10\n",
      "\n",
      "\n",
      " The two SUB states are 3 and 8\n",
      "\n",
      "\n",
      " The two states are 3 and 8\n",
      "\n",
      "\n",
      " The two SUB states are 3 and 9\n",
      "\n",
      "\n",
      " The two states are 3 and 9\n",
      "\n",
      "\n",
      " The two SUB states are 3 and 3\n",
      "\n",
      "\n",
      " The two states are 3 and 3\n",
      "\n",
      "\n",
      " The two SUB states are 3 and 3\n",
      "\n",
      "\n",
      " The two states are 3 and 3\n",
      "\n",
      "\n",
      " The two SUB states are 3 and 3\n",
      "\n",
      "\n",
      " The two states are 3 and 3\n",
      "\n",
      "The amount of states is 11\n",
      "\n",
      "\n",
      "The amount of arcs is 10\n",
      "\n",
      "\n",
      "Different transitions of the Tree: \n",
      "0 --> b/1 --> 0\n",
      "0 --> a/1 --> 3\n",
      "3 --> a/0 --> 3\n",
      "3 --> a/0 --> 3\n",
      "0 --> a/1 --> 3\n",
      "3 --> b/0 --> 0\n",
      "0 --> b/1 --> 0\n",
      "0 --> a/1 --> 3\n",
      "3 --> a/0 --> 3\n",
      "3 --> a/0 --> 3\n"
     ]
    }
   ],
   "source": [
    "#fsm.merge_states(2,4)\n",
    "#fsm.merge_states(0,1)\n",
    "#fsm.merge_states(3,6)\n",
    "#fsm.merge_states(0,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.         1.         0.99067566 0.99067566 0.99067566 0.99067566\n",
      "  0.99067566 1.         0.99067566 0.99067566 1.        ]\n",
      " [1.         1.         0.99067566 0.99067566 0.99067566 0.99067566\n",
      "  0.99067566 1.         0.99067566 0.99067566 1.        ]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [1.         1.         0.99067566 0.99067566 0.99067566 0.99067566\n",
      "  0.99067566 1.         0.99067566 0.99067566 1.        ]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [0.99067566 0.99067566 1.         1.         1.         1.\n",
      "  1.         0.99067566 1.         1.         0.99067566]\n",
      " [1.         1.         0.99067566 0.99067566 0.99067566 0.99067566\n",
      "  0.99067566 1.         0.99067566 0.99067566 1.        ]], shape=(11, 11), dtype=float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for i in range(states.shape[0]):\\n    for j in range(i):\\n        pass_ = False\\n        if(i == j):\\n            continue\\n        for x in fsm.getInpOut(i):\\n            for y in fsm.getInpOut(j):\\n                if(x[0] == y[0] and x[1] != y[1]):\\n                    pass_ = True\\n        #print(f'--we have {i} and {j} and the similarity {sim[i][j]}')\\n        if pass_:\\n            continue\\n        \\n        if(sim1[i][j] >= threshold):\\n            print(f'The states to merge {i} and {j}')\\n            fusionable = True\\n            total += 1\\n            res = fsm.merge_states(i, j)\\n            #pruned += 1 - res\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine(h1, h2):\n",
    "    cos = 0\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    assert len(h1) == len(h2)\n",
    "    for i in range(len(h1)):\n",
    "        cos += h1[i]*h2[i]\n",
    "        s1 += h1[i]**2\n",
    "        s2 += h2[i]**2\n",
    "    s1 = s1**(1/2)\n",
    "    s2 = s2**(1/2)\n",
    "    return cos/(s1*s2)\n",
    "\n",
    "\n",
    "mask = [[True, True, True, False, False],\n",
    "        [True, True, False, False, False],\n",
    "        [True, True, False, False, False],\n",
    "        [True, True, True, True, False],\n",
    "        [True, True, True, True, True],\n",
    "        [True, True, True, False, False],\n",
    "        [True, True, True, True, True],\n",
    "        [True, True, True, True, False]]\n",
    "labels  = np.array([[0,1,1,2,2],\n",
    "        [0,1,2,2,2],\n",
    "        [0,1,2,2,2],\n",
    "        [0,1,1,0,2],\n",
    "        [0,1,1,0,0],\n",
    "        [0,1,0,2,2],\n",
    "        [0,1,0,1,0],\n",
    "        [0,1,0,1,2]])\n",
    "representations = np.array([[[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.67, 1.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.1, 3.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.3, 4.5], [2.3, 4.5], [2.67, 1.01]]])\n",
    "idx = [[0,1,2], \n",
    "    [0,1],\n",
    "    [0,3],\n",
    "    [0,1,2,4],\n",
    "    [0,1,2,4,5],\n",
    "    [0,3,6],\n",
    "    [0,3,7,8,9],\n",
    "    [0,3,7,10]] # maps strings to states\n",
    "n_states = 11\n",
    "states = np.zeros((n_states, 2))\n",
    "states_mask = np.zeros(n_states)\n",
    "\n",
    "for i, _r in enumerate(representations):\n",
    "    #print(_r)\n",
    "    states[idx[i]] = _r[mask[i]]\n",
    "    states_mask[idx[i]] = labels[i][mask[i]]\n",
    "\n",
    "sim1 = []\n",
    "for i in range(len(states)):\n",
    "    sim1.append([])\n",
    "    for j in range(len(states)):\n",
    "        sim1[i].append(cosine(states[i], states[j]))\n",
    "\n",
    "sim1 = tf.convert_to_tensor(sim1)\n",
    "#print(sim1)\n",
    "\n",
    "fusionable = True\n",
    "res, pruned = 0, 0\n",
    "threshold = 1\n",
    "total = 0\n",
    "ter = 0\n",
    "\n",
    "\"\"\"while fusionable and ter < 10:\n",
    "    fusionable = False\n",
    "    ter += 1\"\"\"\n",
    "\n",
    "#fsm.print()\n",
    "\n",
    "\"\"\"for i in range(states.shape[0]):\n",
    "    for j in range(i):\n",
    "        pass_ = False\n",
    "        if(i == j):\n",
    "            continue\n",
    "        for x in fsm.getInpOut(i):\n",
    "            for y in fsm.getInpOut(j):\n",
    "                if(x[0] == y[0] and x[1] != y[1]):\n",
    "                    pass_ = True\n",
    "        #print(f'--we have {i} and {j} and the similarity {sim[i][j]}')\n",
    "        if pass_:\n",
    "            continue\n",
    "        \n",
    "        if(sim1[i][j] >= threshold):\n",
    "            print(f'The states to merge {i} and {j}')\n",
    "            fusionable = True\n",
    "            total += 1\n",
    "            res = fsm.merge_states(i, j)\n",
    "            #pruned += 1 - res\"\"\"\n",
    "    \n",
    "\n",
    "#print(states)\n",
    "#print(states_mask)\n",
    "#print(sim[0])\n",
    "#fsm.print()\n",
    "#fsm.removeDuplicate()\n",
    "#print('After Duplicate deletion')\n",
    "#fsm.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_merging(fsm, states, states_mask, threshold = 1.0):\n",
    "\n",
    "    cos = tf.keras.losses.CosineSimilarity(axis=-1)\n",
    "    sim = -cos(states[None, :, :], states[:, None, :])\n",
    "     \n",
    "    total, pruned = 0, 0\n",
    "    fsm_ = deepcopy(fsm)\n",
    "\n",
    "    for i in range(states.shape[0]):\n",
    "        for j in range(i):\n",
    "            pass_ = False\n",
    "            if(i == j):\n",
    "                continue\n",
    "            for x in fsm_.getInpOut(i):\n",
    "                for y in fsm_.getInpOut(j):\n",
    "                    if(x[0] == y[0] and x[1] != y[1]):\n",
    "                        pass_ = True\n",
    "            #print(f'--we have {i} and {j} and the similarity {sim[i][j]}')\n",
    "            if pass_:\n",
    "                continue\n",
    "            \n",
    "            if(sim[i][j] >= threshold):\n",
    "                print(f'The states to merge {i} and {j}')\n",
    "                fusionable = True\n",
    "                total += 1\n",
    "                res = fsm_.merge_states(i, j)\n",
    "                #pruned += 1 - res\n",
    "                \n",
    "    fsm_.removeDuplicate()\n",
    "    fsm_.print()\n",
    "    fsm_.id = str(fsm_.id) + 'min'\n",
    "    return fsm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m     states_mask[idx[i]] \u001b[39m=\u001b[39m labels[i][mask[i]]\n\u001b[0;32m     43\u001b[0m \u001b[39m#cos = tf.keras.losses.CosineSimilarity(axis=-1,reduction=tf.keras.losses.Reduction.NONE)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m#sim = -cos(states[None, :, :], states[:, None, :])\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m merged_fsm \u001b[39m=\u001b[39m cosine_merging(fsm, states, states_mask)\n",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m, in \u001b[0;36mcosine_merging\u001b[1;34m(fsm, states, states_mask, threshold)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m pass_:\n\u001b[0;32m     20\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mif\u001b[39;00m(sim[i][j] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold):\n\u001b[0;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe states to merge \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     fusionable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 0; input has only 0 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "mask = [[True, True, True, False, False],\n",
    "        [True, True, False, False, False],\n",
    "        [True, True, False, False, False],\n",
    "        [True, True, True, True, False],\n",
    "        [True, True, True, True, True],\n",
    "        [True, True, True, False, False],\n",
    "        [True, True, True, True, True],\n",
    "        [True, True, True, True, False]]\n",
    "labels  = np.array([[0,1,1,2,2],\n",
    "        [0,1,2,2,2],\n",
    "        [0,1,2,2,2],\n",
    "        [0,1,1,0,2],\n",
    "        [0,1,1,0,0],\n",
    "        [0,1,0,2,2],\n",
    "        [0,1,0,1,0],\n",
    "        [0,1,0,1,2]])\n",
    "representations = np.array([[[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.67, 1.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.1, 3.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.1, 3.01], [2.67, 1.01], [2.67, 1.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.3, 4.5], [2.1, 3.01], [2.1, 3.01]],\n",
    "                   [[2.3, 4.5], [2.1, 3.01], [2.3, 4.5], [2.3, 4.5], [2.67, 1.01]]])\n",
    "idx = [[0,1,2], \n",
    "    [0,1],\n",
    "    [0,3],\n",
    "    [0,1,2,4],\n",
    "    [0,1,2,4,5],\n",
    "    [0,3,6],\n",
    "    [0,3,7,8,9],\n",
    "    [0,3,7,10]] # maps strings to states\n",
    "n_states = 11\n",
    "states = np.zeros((n_states, 2))\n",
    "states_mask = np.zeros(n_states)\n",
    "\n",
    "for i, _r in enumerate(representations):\n",
    "    #print(_r)\n",
    "    states[idx[i]] = _r[mask[i]]\n",
    "    states_mask[idx[i]] = labels[i][mask[i]]\n",
    "\n",
    "#cos = tf.keras.losses.CosineSimilarity(axis=-1,reduction=tf.keras.losses.Reduction.NONE)\n",
    "#sim = -cos(states[None, :, :], states[:, None, :])\n",
    "\n",
    "merged_fsm = cosine_merging(fsm, states, states_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'110'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_fsm.return_output('bab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_corpus = ['ba', 'b', 'a', 'baa', 'a', 'baaa', 'aa', 'b', 'abaa', 'abb', 'bb', 'abb', 'aaaa', 'baaaaab', 'abababa']\n",
    "_labels = ['11', '1', '1', '110', '1', '1100', '10', '1', '1010', '101', '11', '101', '1000', '1100000', '1010101']\n",
    "def score_whole_words(mealy, dataset, labels):\n",
    "    acc = 0\n",
    "    for word, y in zip(dataset, labels):\n",
    "        acc += (mealy.return_output(word) == y)\n",
    "    return (acc / len(dataset) * 100)\n",
    "\n",
    "_acc = score_whole_words(merged_fsm, _corpus, _labels)\n",
    "_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(11, 11), dtype=float64, numpy=\n",
       "array([[1.        , 1.        , 0.99067566, 0.99067566, 0.99067566,\n",
       "        0.99067566, 0.99067566, 1.        , 0.99067566, 0.99067566,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 0.99067566, 0.99067566, 0.99067566,\n",
       "        0.99067566, 0.99067566, 1.        , 0.99067566, 0.99067566,\n",
       "        1.        ],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [1.        , 1.        , 0.99067566, 0.99067566, 0.99067566,\n",
       "        0.99067566, 0.99067566, 1.        , 0.99067566, 0.99067566,\n",
       "        1.        ],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [0.99067566, 0.99067566, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99067566, 1.        , 1.        ,\n",
       "        0.99067566],\n",
       "       [1.        , 1.        , 0.99067566, 0.99067566, 0.99067566,\n",
       "        0.99067566, 0.99067566, 1.        , 0.99067566, 0.99067566,\n",
       "        1.        ]])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert 1e-12 to EagerTensor of dtype int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m], [\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m]])\n\u001b[0;32m      2\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m2.1\u001b[39m, \u001b[39m3.2\u001b[39m], [\u001b[39m4.8\u001b[39m, \u001b[39m5.7\u001b[39m]])\n\u001b[1;32m----> 3\u001b[0m sim \u001b[39m=\u001b[39m cos(a[\u001b[39mNone\u001b[39;49;00m, :, :], a[:, \u001b[39mNone\u001b[39;49;00m, :])\n\u001b[0;32m      4\u001b[0m sim\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py:142\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    140\u001b[0m     )\n\u001b[1;32m--> 142\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[0;32m    144\u001b[0m in_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(y_pred)\n\u001b[0;32m    145\u001b[0m out_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(losses)\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py:268\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    261\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[0;32m    262\u001b[0m         y_pred, y_true\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    265\u001b[0m ag_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m    266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    267\u001b[0m )\n\u001b[1;32m--> 268\u001b[0m \u001b[39mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fn_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Gael\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\losses.py:2461\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(y_true, y_pred, axis)\u001b[0m\n\u001b[0;32m   2420\u001b[0m \u001b[39m@keras_export\u001b[39m(\n\u001b[0;32m   2421\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkeras.losses.cosine_similarity\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2422\u001b[0m     v1\u001b[39m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2430\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2431\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcosine_similarity\u001b[39m(y_true, y_pred, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   2432\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the cosine similarity between labels and predictions.\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m \n\u001b[0;32m   2434\u001b[0m \u001b[39m    Note that it is a number between -1 and 1. When it is a negative number\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2459\u001b[0m \u001b[39m      Cosine similarity tensor.\u001b[39;00m\n\u001b[0;32m   2460\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2461\u001b[0m     y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49ml2_normalize(y_true, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   2462\u001b[0m     y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39ml2_normalize(y_pred, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m   2463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(y_true \u001b[39m*\u001b[39m y_pred, axis\u001b[39m=\u001b[39maxis)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert 1e-12 to EagerTensor of dtype int32"
     ]
    }
   ],
   "source": [
    "a = np.array([[2,3], [5,6]])\n",
    "b = np.array([[2.1, 3.2], [4.8, 5.7]])\n",
    "sim2 = cos(a[None, :, :], a[:, None, :])\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rer = [['a','0'],['b','1']]\n",
    "rerr = []\n",
    "sas = []\n",
    "if all(x in rer for x in rerr):\n",
    "    print(True)\n",
    "#rer = [set(x) for x in rer]\n",
    "#rerr = [set(x) for x in rerr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rer \u001b[39m=\u001b[39m (\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m rer \u001b[39m=\u001b[39m rer[\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m]\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(\u001b[39mlist\u001b[39m(rer)))\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
